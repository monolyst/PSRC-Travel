{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSRC Pre-processing\n",
    "The main purpose of these scripts is to create an merged analytical data set from trip, household, \n",
    "and person data sets. The analytical data set is denormalized at the 'trip' level and can be\n",
    "found at DATA_DIR + 'Trip_Household_Merged.csv'. A simple tableau dashboard built from this set (currently\n",
    "draft and needs to be upoaded with the revised data) can be accessed at:\n",
    "https://public.tableau.com/views/PSRC2018/ODPairs?:embed=y&:display_count=yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first line from .csv, only needed once! Alternatively, load strait from .XLS and ignore the first row.\n",
    "\n",
    "def removeFirstLine():\n",
    "    data_dir = '../data/'\n",
    "\n",
    "    with open(data_dir + '2017-pr2-1-household.csv', 'r') as fin:\n",
    "        data = fin.read().splitlines(True)\n",
    "    with open(data_dir + '2017-pr2-1-household.csv', 'w') as fout:\n",
    "        fout.writelines(data[1:])\n",
    "    \n",
    "    with open(data_dir + '2017-pr2-5-trip.csv', 'r') as fin:\n",
    "        data = fin.read().splitlines(True)\n",
    "    with open(data_dir + '2017-pr2-5-trip.csv', 'w') as fout:\n",
    "        fout.writelines(data[1:])\n",
    "              \n",
    "    with open(data_dir + '2017-pr2-2-person.csv', 'r') as fin:\n",
    "        data = fin.read().splitlines(True)\n",
    "    with open(data_dir + '2017-pr2-2-person.csv', 'w') as fout:\n",
    "        fout.writelines(data[1:])\n",
    "              \n",
    "#removeFirstLine()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Persons (Race demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     personid  race_afam  race_aiak  race_asian  race_hapi  race_hisp  \\\n",
      "0  1710000501        0.0        0.0         0.0        0.0        0.0   \n",
      "1  1710000502        0.0        0.0         0.0        0.0        0.0   \n",
      "2  1710002401        0.0        0.0         0.0        0.0        0.0   \n",
      "3  1710002402        0.0        0.0         0.0        0.0        0.0   \n",
      "5  1710005201        0.0        0.0         0.0        0.0        0.0   \n",
      "\n",
      "   race_white  race_other  gender  gender_male  gender_female  gender_another  \\\n",
      "0         1.0         0.0       2            0              1               0   \n",
      "1         1.0         0.0       1            1              0               0   \n",
      "2         1.0         0.0       3            0              0               1   \n",
      "3         1.0         0.0       2            0              1               0   \n",
      "5         1.0         0.0       2            0              1               0   \n",
      "\n",
      "   gender_noanswer  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "5                0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Load race and gender information from the Persons data\n",
    "df_Person = pd.read_csv(DATA_DIR + '2017-pr2-2-person.csv')\n",
    "#print(df_Person.dtypes)\n",
    "\n",
    "df_persons = df_Person[['personid','race_afam','race_aiak','race_asian','race_hapi','race_hisp','race_white','race_other','gender']]\n",
    "df_persons.dropna(axis = 0, inplace=True)\n",
    "\n",
    "df_persons['gender_male'] = np.where(df_persons['gender']==1, 1, 0)\n",
    "df_persons['gender_female'] = np.where(df_persons['gender']==2, 1, 0)\n",
    "df_persons['gender_another'] = np.where(df_persons['gender']==3, 1, 0)\n",
    "df_persons['gender_noanswer'] = np.where(df_persons['gender']==4, 1, 0)\n",
    "\n",
    "print (df_persons.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  final_home_bg      hhid  hhsize  vehicle_count  numchildren  hhincome_broad  \\\n",
      "0  530330323234  17100005       2              2            0               4   \n",
      "1  530330076002  17100024       3              1            1               4   \n",
      "2  530330075005  17100052       1              0            0               1   \n",
      "3  530330084001  17100059       1              0            0               5   \n",
      "4  530530720002  17100060       1              1            0               1   \n",
      "\n",
      "   car_share  rent_own  res_dur  offpark  hh_wt_revised            hh_uv  \\\n",
      "0          2         1        7        2      24.441709  Outside Seattle   \n",
      "1          1         1        2        1      26.224981   Madison-Miller   \n",
      "2          2         2        6        1      25.692826     Capitol Hill   \n",
      "3          2         2        1        0      47.768728        Pike/Pine   \n",
      "4          2         2        5        2     278.147224  Outside Seattle   \n",
      "\n",
      "                  hh_uv_type            income homeownership  \\\n",
      "0            Outside Seattle   $75,000-$99,999           Own   \n",
      "1  Residential Urban Village   $75,000-$99,999           Own   \n",
      "2       Urban Center Village     Under $25,000          Rent   \n",
      "3       Urban Center Village  $100,000 or more          Rent   \n",
      "4            Outside Seattle     Under $25,000          Rent   \n",
      "\n",
      "                    tenure  \n",
      "0       More than 20 years  \n",
      "1    Between 1 and 2 years  \n",
      "2  Between 10 and 20 years  \n",
      "3         Less than a year  \n",
      "4   Between 5 and 10 years  \n"
     ]
    }
   ],
   "source": [
    "df_Household = pd.read_csv(DATA_DIR + '2017-pr2-1-household.csv')\n",
    "df_Blockgroup_UrbanVillage = pd.read_csv(DATA_DIR + 'Blockgroup_UrbanVillage.csv')\n",
    "#print(df_Person.dtypes)\n",
    "\n",
    "df_households = df_Household[['final_home_bg','hhid','hhsize','vehicle_count','numchildren',\n",
    "                              'hhincome_broad','car_share','rent_own','res_dur','offpark','hh_wt_revised']]\n",
    "\n",
    "df_households['final_home_bg'] = df_households['final_home_bg'].astype(float).astype(int).astype(str)\n",
    "\n",
    "# merge with seattle block group data\n",
    "df_seattle = df_Blockgroup_UrbanVillage[['BLOCKGROUP','URBAN_VILLAGE_NAME','URBAN_VILLAGE_TYPE']]\n",
    "df_seattle['final_home_bg'] = df_seattle['BLOCKGROUP'].astype(str)\n",
    "\n",
    "df_households = pd.merge(left=df_households, right=df_seattle, how='left', on='final_home_bg')\n",
    "df_households.drop(['BLOCKGROUP'], axis = 1, inplace=True)\n",
    "df_households['URBAN_VILLAGE_NAME'] = df_households['URBAN_VILLAGE_NAME'].fillna(\"Outside Seattle\")\n",
    "df_households['URBAN_VILLAGE_TYPE'] = df_households['URBAN_VILLAGE_TYPE'].fillna(\"Outside Seattle\")\n",
    "\n",
    "df_households = df_households.rename(columns={'final_lat':'hh_lat', 'final_lng':'hh_lng'})\n",
    "df_households = df_households.rename(columns={'URBAN_VILLAGE_NAME':'hh_uv', 'URBAN_VILLAGE_TYPE':'hh_uv_type'})\n",
    "\n",
    "# Assign income variables\n",
    "df_households['income'] = np.where(df_households['hhincome_broad']==1, \"Under $25,000\", \"\")\n",
    "df_households['income'] = np.where(df_households['hhincome_broad']==2, \"$25,000-$49,999\", df_households['income'])\n",
    "df_households['income'] = np.where(df_households['hhincome_broad']==3, \"$50,000-$74,999\", df_households['income'])\n",
    "df_households['income'] = np.where(df_households['hhincome_broad']==4, \"$75,000-$99,999\", df_households['income'])\n",
    "df_households['income'] = np.where(df_households['hhincome_broad']==5, \"$100,000 or more\", df_households['income'])\n",
    "df_households['income'] = np.where(df_households['hhincome_broad']==6, \"Prefer not to answer\", df_households['income'])\n",
    "\n",
    "# Assign home ownership\n",
    "df_households['homeownership'] = np.where(df_households['rent_own']==1, \"Own\", \"Other\")\n",
    "df_households['homeownership'] = np.where(df_households['rent_own']==2, \"Rent\", df_households['homeownership'])\n",
    "\n",
    "# Assign residency tenure\n",
    "df_households['tenure'] = np.where(df_households['res_dur']==1, \"Less than a year\", \"\")\n",
    "df_households['tenure'] = np.where(df_households['res_dur']==2, \"Between 1 and 2 years\", df_households['tenure'])\n",
    "df_households['tenure'] = np.where(df_households['res_dur']==3, \"Between 2 and 3 years\", df_households['tenure'])\n",
    "df_households['tenure'] = np.where(df_households['res_dur']==4, \"Between 3 and 5 years\", df_households['tenure'])\n",
    "df_households['tenure'] = np.where(df_households['res_dur']==5, \"Between 5 and 10 years\", df_households['tenure'])\n",
    "df_households['tenure'] = np.where(df_households['res_dur']==6, \"Between 10 and 20 years\", df_households['tenure'])\n",
    "df_households['tenure'] = np.where(df_households['res_dur']==7, \"More than 20 years\", df_households['tenure'])\n",
    "\n",
    "#df_households = pd.merge(left=df_households, right=df_race, how='left', on='hhid')\n",
    "#df_households = pd.merge(left=df_households, right=df_gender, how='left', on='hhid')\n",
    "\n",
    "print (df_households.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (17,18,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,68,69,80,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recid', 'hhid', 'personid', 'pernum', 'tripid', 'tripnum', 'tottrip1', 'tottrip2', 'daynum', 'hhgroup', 'origin_home', 'dest_home', 'depart_time_mam', 'depart_time_hhmm', 'arrival_time_mam', 'arrival_time_hhmm', 'reported_duration', 'google_duration', 'trip_path_distance', 'speed_mph', 'activity_duration', 'origin_purpose', 'o_purpose_other', 'dest_purpose', 'dest_purpose_other', 'mode_1', 'mode_2', 'mode_3', 'mode_4', 'travelers_hh', 'travelers_nonhh', 'travelers_total', 'driver', 'hhmember1', 'hhmember2', 'hhmember3', 'hhmember4', 'hhmember5', 'hhmember6', 'hhmember7', 'hhmember8', 'hhmember9', 'hhmember_none', 'pool_start', 'change_vehicles', 'park_ride_area_start', 'park_ride_area_end', 'park_ride_lot_start', 'park_ride_lot_end', 'toll', 'toll_pay', 'taxi_type', 'taxi_pay', 'bus_type', 'bus_pay', 'bus_cost_dk', 'ferry_type', 'ferry_pay', 'ferry_cost_dk', 'rail_type', 'rail_pay', 'rail_cost_dk', 'air_type', 'air_pay', 'airfare_cost_dk', 'mode_acc', 'mode_egr', 'park', 'park_type', 'park_pay', 'transit_system_1', 'transit_system_2', 'transit_system_3', 'transit_system_4', 'transit_system_5', 'transit_line_1', 'transit_line_2', 'transit_line_3', 'transit_line_4', 'transit_line_5', 'svy_complete', 'traveldate', 'dayofweek', 'copied_trip', 'completed_at', 'revised_at', 'revised_count', 'google_duration-SEC', 'trip_path_distance-mtr', 'user_merged', 'user_split', 'analyst_merged', 'analyst_split', 'flag_teleport', 'proxy_added_trip', 'nonproxy_derived_trip', 'child_trip_location_tripid', 'num_trips_linked', 'combined_modes', 'hh_day_complete', 'o_tract', 'o_bg', 'o_puma10', 'o_rgcname', 'o_taz2010', 'd_tract', 'd_bg', 'd_puma10', 'd_rgcname', 'd_taz2010', 'nwkdays', 'hh_day_wt_revised', 'trip_weight_revised']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tripid      hhid depart_time_hhmm arrival_time_hhmm          o_bg  \\\n",
      "0  1710000501001  17100005          9:55 AM          10:05 AM  530330323234   \n",
      "1  1710000501002  17100005         10:10 AM          10:15 AM  530330323132   \n",
      "2  1710000501003  17100005         10:50 AM          11:00 AM  530330323133   \n",
      "3  1710000501004  17100005         11:35 AM          12:00 PM  530330323234   \n",
      "4  1710000501005  17100005          3:10 PM           3:30 PM  530330232022   \n",
      "\n",
      "           d_bg    personid  google_duration  trip_path_distance  daynum  \\\n",
      "0  530330323132  1710000501              7.0                2.30       1   \n",
      "1  530330323133  1710000501              4.0                1.12       1   \n",
      "2  530330323234  1710000501              7.0                3.26       1   \n",
      "3  530330232022  1710000501             18.0                8.13       1   \n",
      "4  530330323234  1710000501             16.0                8.04       1   \n",
      "\n",
      "       ...               uv_dest      uvType_dest  \\\n",
      "0      ...       Outside Seattle  Outside Seattle   \n",
      "1      ...       Outside Seattle  Outside Seattle   \n",
      "2      ...       Outside Seattle  Outside Seattle   \n",
      "3      ...       Outside Seattle  Outside Seattle   \n",
      "4      ...       Outside Seattle  Outside Seattle   \n",
      "\n",
      "                           uv_od_pair                    bg_od_pair  \\\n",
      "0  Outside Seattle to Outside Seattle  530330323234 to 530330323132   \n",
      "1  Outside Seattle to Outside Seattle  530330323132 to 530330323133   \n",
      "2  Outside Seattle to Outside Seattle  530330323133 to 530330323234   \n",
      "3  Outside Seattle to Outside Seattle  530330323234 to 530330232022   \n",
      "4  Outside Seattle to Outside Seattle  530330232022 to 530330323234   \n",
      "\n",
      "             mode  drive_alone  purpose depart_time depart_day depart_period  \n",
      "0  Drive w Others            0    Other         9.0          2    Weekday AM  \n",
      "1  Drive w Others            0    Other        10.0          2   Weekday Mid  \n",
      "2  Drive w Others            0  Go Home        10.0          2   Weekday Mid  \n",
      "3     Drive Alone            1    Other        11.0          2   Weekday Mid  \n",
      "4     Drive Alone            1  Go Home        15.0          2   Weekday Mid  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "df_Trip = pd.read_csv(DATA_DIR + '2017-pr2-5-trip.csv')\n",
    "print (list(df_Trip.columns.values))\n",
    "\n",
    "df_trips = df_Trip[['tripid','hhid', 'depart_time_hhmm','arrival_time_hhmm',\n",
    "                    'o_bg','d_bg','personid','google_duration','trip_path_distance',\n",
    "                    'daynum','origin_purpose','dest_purpose',\n",
    "                    'mode_1','travelers_total','traveldate','trip_weight_revised']]\n",
    "\n",
    "#print(df_trips.dtypes)\n",
    "\n",
    "# drop null blockgroups\n",
    "df_trips = df_trips.dropna(subset=['o_bg', 'd_bg'])\n",
    "\n",
    "df_trips['d_bg'] = df_trips['d_bg'].astype(int).astype(str)\n",
    "df_trips['o_bg'] = df_trips['o_bg'].astype(int).astype(str)\n",
    "\n",
    "# merge with seattle block group data\n",
    "df_seattle = df_Blockgroup_UrbanVillage[['BLOCKGROUP','URBAN_VILLAGE_NAME','URBAN_VILLAGE_TYPE']]\n",
    "df_seattle['BLOCKGROUP'] = df_seattle['BLOCKGROUP'].astype(str)\n",
    "                              \n",
    "df_trips = pd.merge(left=df_trips, right=df_seattle, how='left', left_on='o_bg', right_on='BLOCKGROUP')\n",
    "df_trips = df_trips.rename(columns={'URBAN_VILLAGE_NAME':'uv_origin', 'URBAN_VILLAGE_TYPE':'uvType_origin'})\n",
    "df_trips.drop(['BLOCKGROUP'], axis = 1, inplace=True)\n",
    "\n",
    "df_trips['uv_origin'] = df_trips['uv_origin'].fillna(\"Outside Seattle\")\n",
    "df_trips['uvType_origin'] = df_trips['uvType_origin'].fillna(\"Outside Seattle\")\n",
    "\n",
    "df_trips = pd.merge(left=df_trips, right=df_seattle, how='left', left_on='d_bg', right_on='BLOCKGROUP')\n",
    "df_trips = df_trips.rename(columns={'URBAN_VILLAGE_NAME':'uv_dest', 'URBAN_VILLAGE_TYPE':'uvType_dest'})\n",
    "df_trips.drop(['BLOCKGROUP'], axis = 1, inplace=True)\n",
    "\n",
    "df_trips['uv_dest'] = df_trips['uv_dest'].fillna(\"Outside Seattle\")\n",
    "df_trips['uvType_dest'] = df_trips['uvType_dest'].fillna(\"Outside Seattle\")\n",
    "\n",
    "# Drop missing variables, clean up column\n",
    "df_trips['mode_1'] = df_trips['mode_1'].fillna(0)\n",
    "df_trips['mode_1'] = df_trips['mode_1'].astype(str).replace(' ', '0')\n",
    "df_trips['mode_1'] = df_trips['mode_1'].astype(str).astype(int)\n",
    "df_trips['travelers_total'] = df_trips['travelers_total'].astype(str).replace(' ', '0')\n",
    "df_trips['travelers_total'] = df_trips['travelers_total'].astype(str).astype(int)\n",
    "\n",
    "# drop rows where the duration or distance is null or an empty space\n",
    "df_trips = df_trips[df_trips['google_duration'].notnull()]\n",
    "df_trips = df_trips[df_trips['trip_path_distance'].notnull()]\n",
    "df_trips = df_trips[df_trips['google_duration'] != \" \"]\n",
    "df_trips = df_trips[df_trips['trip_path_distance'] != \" \"]\n",
    "df_trips['google_duration'] = df_trips['google_duration'].astype(float)\n",
    "df_trips['trip_path_distance'] = df_trips['trip_path_distance'].astype(float)\n",
    "df_trips = df_trips[df_trips['trip_path_distance'].notnull()]\n",
    "\n",
    "# Create OD Pairs for urban villages and block groups\n",
    "df_trips['uv_od_pair'] = df_trips['uv_origin'] + \" to \" + df_trips['uv_dest']\n",
    "df_trips['bg_od_pair'] = df_trips['o_bg'].astype(str) + \" to \" + df_trips['d_bg'].astype(str)\n",
    "\n",
    "# Assign mode variables\n",
    "df_trips['mode'] = np.where(df_trips['mode_1']==1, \"Walk\", \"Other\")\n",
    "df_trips['mode'] = np.where(df_trips['mode_1']==2, \"Bike\", df_trips['mode'])\n",
    "\n",
    "df_trips['mode'] = np.where((df_trips['mode_1']>=3) & (df_trips['mode_1']<=17)  & (df_trips['travelers_total']==1), \"Drive Alone\", df_trips['mode'])\n",
    "df_trips['mode'] = np.where(((df_trips['mode_1']==21) | (df_trips['mode_1']==22) |\n",
    "                                 (df_trips['mode_1']==33) | (df_trips['mode_1']==34) | (df_trips['mode_1']==18))\n",
    "                                 & (df_trips['travelers_total']==1), \"Drive Alone\", df_trips['mode'])\n",
    "\n",
    "df_trips['mode'] = np.where((df_trips['mode_1']>=3) & (df_trips['mode_1']<=17)  & (df_trips['travelers_total']!=1), \"Drive w Others\", df_trips['mode'])\n",
    "df_trips['mode'] = np.where(((df_trips['mode_1']==21) | (df_trips['mode_1']==22) |\n",
    "                                 (df_trips['mode_1']==33) | (df_trips['mode_1']==34) | (df_trips['mode_1']==18))\n",
    "                                 & (df_trips['travelers_total']>1), \"Drive w Others\", df_trips['mode'])\n",
    "\n",
    "df_trips['mode'] = np.where((df_trips['mode_1']==23) | (df_trips['mode_1']==41) | (df_trips['mode_1']==42), \"Transit\", df_trips['mode'])\n",
    "df_trips['mode'] = np.where((df_trips['mode_1']==32) | (df_trips['travelers_total']==52), \"Transit\", df_trips['mode'])\n",
    "\n",
    "df_trips['drive_alone'] = np.where(df_trips['mode']==\"Drive Alone\", 1, 0)\n",
    "\n",
    "\n",
    "# Assign purpose variables\n",
    "df_trips['purpose'] = np.where(df_trips['dest_purpose']==1, \"Go Home\", \"Other\")\n",
    "df_trips['purpose'] = np.where(df_trips['dest_purpose']==6, \"School\", df_trips['purpose'])\n",
    "df_trips['purpose'] = np.where((df_trips['dest_purpose']==10) | (df_trips['dest_purpose']==11), \"Work\", df_trips['purpose'])\n",
    "\n",
    "# Assign time period variables\n",
    "df_trips['depart_time'] = pd.to_datetime(df_trips['depart_time_hhmm'], errors='coerce')\n",
    "df_trips['depart_day'] = pd.to_datetime(df_trips['traveldate'], errors='coerce')\n",
    "\n",
    "df_trips['depart_day'] = df_trips['depart_day'].dt.dayofweek\n",
    "df_trips['depart_time'] = df_trips['depart_time'].dt.hour\n",
    "\n",
    "df_trips['depart_period'] = np.where((df_trips['depart_day']>=0) & (df_trips['depart_day']<=4) & \n",
    "                                     (df_trips['depart_time']>=7) & (df_trips['depart_time']<=9), \"Weekday AM\", \"\")\n",
    "\n",
    "df_trips['depart_period'] = np.where((df_trips['depart_day']>=0) & (df_trips['depart_day']<=4) & \n",
    "                                     (df_trips['depart_time']>=10) & (df_trips['depart_time']<=15), \"Weekday Mid\", df_trips['depart_period'])\n",
    "\n",
    "df_trips['depart_period'] = np.where((df_trips['depart_day']>=0) & (df_trips['depart_day']<=4) & \n",
    "                                     (df_trips['depart_time']>=16) & (df_trips['depart_time']<=19), \"Weekday PM\", df_trips['depart_period'])\n",
    "\n",
    "df_trips['depart_period'] = np.where((df_trips['depart_day']>=0) & (df_trips['depart_day']<=4) & \n",
    "                                     (df_trips['depart_time']<7), \"Other\", df_trips['depart_period'])\n",
    "\n",
    "df_trips['depart_period'] = np.where((df_trips['depart_day']>=0) & (df_trips['depart_day']<=4) & \n",
    "                                     (df_trips['depart_time']>19), \"Late Night\", df_trips['depart_period'])\n",
    "\n",
    "df_trips['depart_period'] = np.where((df_trips['depart_day']>=5) & (df_trips['depart_day']<=6), \"Late Night\", df_trips['depart_period'])\n",
    "\n",
    "#dts = dfBad[cols].apply(lambda x: pd.to_datetime(x, errors='coerce', format='%m/%d/%Y'))\n",
    "#pd.to_datetime(df_trips['depart_time_timestamp'])\n",
    "\n",
    "#df_trips['depart_time_timestamp'] = datetime.strptime(df_trips['depart_time_timestamp'], '%b %d %Y %I:%M%p')\n",
    "#to_datetime\n",
    "\n",
    "#print (df_trips.dtypes)\n",
    "print (df_trips.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged, Normalized Trip Dataset\n",
    "Add household and person information to each trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tripid      hhid depart_time_hhmm arrival_time_hhmm          o_bg  \\\n",
      "0  1710000501001  17100005          9:55 AM          10:05 AM  530330323234   \n",
      "1  1710000501002  17100005         10:10 AM          10:15 AM  530330323132   \n",
      "2  1710000501003  17100005         10:50 AM          11:00 AM  530330323133   \n",
      "3  1710000501004  17100005         11:35 AM          12:00 PM  530330323234   \n",
      "4  1710000501005  17100005          3:10 PM           3:30 PM  530330232022   \n",
      "\n",
      "           d_bg    personid  google_duration  trip_path_distance  daynum  \\\n",
      "0  530330323132  1710000501              7.0                2.30       1   \n",
      "1  530330323133  1710000501              4.0                1.12       1   \n",
      "2  530330323234  1710000501              7.0                3.26       1   \n",
      "3  530330232022  1710000501             18.0                8.13       1   \n",
      "4  530330323234  1710000501             16.0                8.04       1   \n",
      "\n",
      "        ...        race_asian  race_hapi  race_hisp  race_white race_other  \\\n",
      "0       ...               0.0        0.0        0.0         1.0        0.0   \n",
      "1       ...               0.0        0.0        0.0         1.0        0.0   \n",
      "2       ...               0.0        0.0        0.0         1.0        0.0   \n",
      "3       ...               0.0        0.0        0.0         1.0        0.0   \n",
      "4       ...               0.0        0.0        0.0         1.0        0.0   \n",
      "\n",
      "   gender gender_male gender_female gender_another gender_noanswer  \n",
      "0     2.0         0.0           1.0            0.0             0.0  \n",
      "1     2.0         0.0           1.0            0.0             0.0  \n",
      "2     2.0         0.0           1.0            0.0             0.0  \n",
      "3     2.0         0.0           1.0            0.0             0.0  \n",
      "4     2.0         0.0           1.0            0.0             0.0  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "df_trip_household= pd.merge(left=df_trips, right=df_households, how='left', left_on='hhid', right_on='hhid')\n",
    "df_trip_household= pd.merge(left=df_trip_household, right=df_persons, how='left', left_on='personid', right_on='personid')\n",
    "\n",
    "print (df_trip_household.head())\n",
    "\n",
    "df_trip_household.to_csv(DATA_DIR + 'Trip_Household_Merged.csv', mode='w', header=True, index=False)\n",
    "#df_UV_Origins.to_csv(data_dir + 'UV_Origins.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blockgroup Pairs\n",
    "This section calculates all combinations of blockgroups pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load King County blockgroup file that has centroids.\n",
    "\n",
    "df_Blockgroups = pd.read_csv(data_dir + 'KingCountyCentroids.csv') \n",
    "df_Blockgroups['geoid'] = df_Blockgroups['geoid'].astype(str)\n",
    "\n",
    "df_Merged= pd.merge(left=df_trips, right=df_Blockgroups, how='inner', left_on='d_bg', right_on='geoid')\n",
    "#print (list(df_Merged.columns.values))\n",
    "\n",
    "df_Merged['bg_pair'] = df_Merged['o_bg'].astype(str) + \"-\" + df_Merged['d_bg'].astype(str)\n",
    "\n",
    "df_Merged_agg = df_Merged.groupby(['bg_pair'], as_index=False).agg({'o_bg':['max'],'d_bg':['max'],\n",
    "                                                                   'centroid lat':['max'],'centroid long':['max']})\n",
    "df_Merged_agg.columns = df_Merged_agg.columns.droplevel(level=1)\n",
    "df_Merged_agg = df_Merged_agg.rename(index=str, columns={\"centroid lat\": \"lat_dest\", \"centroid long\": \"lon_dest\"})\n",
    "\n",
    "df_Merged_agg= pd.merge(left=df_Merged_agg, right=df_Blockgroups, how='inner', left_on='o_bg', right_on='geoid')\n",
    "df_Merged_agg = df_Merged_agg.rename(index=str, columns={\"centroid lat\": \"lat_origin\", \"centroid long\": \"lon_origin\"})\n",
    "\n",
    "df_Merged_agg.to_csv(data_dir + 'blockgroup_pairs_kc.csv', mode='w', header=True, index=False)\n",
    "\n",
    "print (df_Merged_agg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
